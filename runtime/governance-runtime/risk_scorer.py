
# ============================================================
#  ORI Governance Runtime — Risk Scoring Engine
#  File: risk_scorer.py
#
#  Description:
#      Computes structured risk scores for model actions,
#      responses, tool calls, and any state transition that
#      interacts with the governance loop.
#
#      This module does NOT make decisions.
#      It only produces normalized, explainable metrics.
#
#  Design Goals (2026 Standard):
#      - Deterministic scoring across agents
#      - Traceable computation path (lineage-safe)
#      - Immutable risk frames (no mutation after emit)
#      - Composable scoring layers
#      - Plug-in structure for new risk heuristics
#
#  Author: ORI 
#  Version: 1.0
#  License: MIT
# ============================================================

from dataclasses import dataclass, field
from typing import Dict, Any


# ============================================================
#  RISK SCORE STRUCTURE
# ============================================================

@dataclass(frozen=True)
class RiskFrame:
    """
    A RiskFrame is an immutable structure that captures the
    normalized risk metrics associated with a single model action.

    Fields:
        score: float
            Final normalized risk score in [0.0, 1.0]
        components: Dict[str, float]
            Individual risk contributions per category
        context: Dict[str, Any]
            Additional metadata (non-identifying, non-user-specific)
            useful for the governance loop or lineage hashing.
    """
    score: float
    components: Dict[str, float] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)


# ============================================================
#  RISK SCORING ENGINE
# ============================================================

class RiskScorer:
    """
    RiskScorer evaluates model output and state transitions
    and assigns a normalized risk score.

    The output is a RiskFrame that contains:
        - overall risk score
        - category-specific risk contributions
        - contextual metadata for lineage tracking

    NOTE:
        This module does NOT evaluate user identity or intentions.
        It ONLY analyzes AI system outputs & internal state mutations.
        No personal data is consumed or inferred.
    """

    # Default heuristic weights (adjustable by policy_resolver)
    WEIGHTS = {
        "safety_boundary": 0.4,
        "coherence_deviation": 0.2,
        "persona_integrity": 0.2,
        "behavioral_drift": 0.2
    }

    def score(self, output: str, *, state: Dict[str, Any]) -> RiskFrame:
        """
        Public scoring interface.

        Parameters:
            output (str):
                The text generated by the model during this step.
            state (dict):
                Internal state snapshot (sanitized / non-personal).

        Returns:
            RiskFrame (immutable)
        """

        components = {
            "safety_boundary": self._score_safety_boundary(output),
            "coherence_deviation": self._score_coherence(output),
            "persona_integrity": self._score_persona_integrity(state),
            "behavioral_drift": self._score_behavior_drift(state),
        }

        normalized = self._normalize(components)

        return RiskFrame(
            score=normalized,
            components=components,
            context={
                "length": len(output),
                "state_hash": state.get("hash", None),
                "transition_id": state.get("transition_id", None),
            }
        )

    # --------------------------------------------------------
    # RISK SUB-COMPONENT SCORERS
    # --------------------------------------------------------

    def _score_safety_boundary(self, output: str) -> float:
        """
        Scores whether the output approaches any forbidden boundary.
        This uses deterministic pattern heuristics (not ML).
        """
        risky_tokens = ["bypass", "override", "exploit", "force_exit"]
        hits = sum(1 for tok in risky_tokens if tok in output.lower())
        return min(hits * 0.25, 1.0)

    def _score_coherence(self, output: str) -> float:
        """
        Measures coherence deviation — if the output drifts into
        nonsensical, contradictory, or destabilized patterns.
        """
        if len(output.strip()) == 0:
            return 0.5
        if any(x in output for x in ["???", "[[", "]]"]):
            return 0.3
        return 0.0

    def _score_persona_integrity(self, state: Dict[str, Any]) -> float:
        """
        Checks whether persona invariants remain stable.
        This relies on persona_integrity_checker, but here
        we approximate using state flags.
        """
        return 1.0 if state.get("persona_violation") else 0.0

    def _score_behavior_drift(self, state: Dict[str, Any]) -> float:
        """
        Detects drift based on internal counters or historical deltas.
        """
        drift_value = state.get("drift_level", 0.0)
        return float(min(max(drift_value, 0.0), 1.0))

    # --------------------------------------------------------
    # NORMALIZATION
    # --------------------------------------------------------

    def _normalize(self, comps: Dict[str, float]) -> float:
        """
        Weighted sum normalization into a unified [0,1] range.
        """
        total = 0.0
        for key, weight in self.WEIGHTS.items():
            total += comps.get(key, 0.0) * weight
        return min(max(total, 0.0), 1.0)


# ============================================================
#  MODULE DEBUG / SELF-TEST (Optional)
# ============================================================

if __name__ == "__main__":
    scorer = RiskScorer()
    test_state = {
        "hash": "abc123",
        "transition_id": "T001",
        "persona_violation": False,
        "drift_level": 0.1,
    }

    frame = scorer.score("Hello, world! No exploits here.", state=test_state)
    print(frame)
